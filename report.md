# Отчёт

### Цель работы
Нам дан датасет, где целевой переменной является доход человека на основании его возраста, пола, образования и прочих признаков.
```
https://huggingface.co/datasets/scikit-learn/adult-census-income/viewer/default/train?row=4
```

Нужно провести серию экспериментов, чтобы понять, как различные параметры влияют на качество обучения.

### Эксперимент 1 
В MLFlow данные этого эксперимента хранятся под A_train_size_{n}, где n - размер тренировочного датасета.

**Гипотеза:**
увеличение размера тренировочного датасета приводит к росту качества модели, поскольку модель обучается на большем количестве примеров и лучше обобщает данные.
Используется logreg с параметрами {'max_iter': 500, 'solver': 'lbfgs', 'C': 1.0}
Используются фичи race,sex,native.country,hours.per.week,occupation,marital.status,education.

**Исследуемый параметр:**
размер тренировочного датасета (train_size).

**Сетка изменения параметра:** 1000 -> 6000 -> 15000 -> 20000 объектов

При проведении эксперимента остальные параметры были зафиксированы.

#### Анализ результатов
Выдвинутая гипотеза не подтвердилась для выбранной модели.
Увеличение размера тренировочного датасета приводит к небольшому росту ROC-AUC, однако после определённого объёма данных наблюдается эффект насыщения - качество модели практически перестаёт улучшаться.
Вероятной причиной является ограниченная выразительная способность линейной модели (Logistic Regression), которая не способна извлечь дополнительную информацию из увеличенного объёма данных без изменения модели или набора признаков.

```
Ссылка на лучший запуск 
http://158.160.2.37:5000/#/experiments/15/runs/fdd2f807bbd041f1813d736fdd1d55f9
```

### Эксперимент 2
В MLFlow данные этого эксперимента хранятся под B_model_type_{model}, где model - используемая модель для предсказаний income.

**Гипотеза:**
более сложные модели, способные учитывать нелинейные зависимости между признаками (деревья решений и ансамблевые методы), будут показывать более высокое качество классификации по сравнению с линейной моделью логистической регрессии при прочих равных (одинаковый train_size и одинаковый набор фичей)


**Исследуемый параметр:**
выбранная модель: model_type 

**Сетка изменения параметра:**: logreg -> decision_tree -> random_forest -> gradient_boosting

#### Анализ результатов
Выдвинутая гипотеза подтвердилась.
Более сложные модели, способные учитывать нелинейные зависимости в данных, значительно превосходят линейную модель логистической регрессии по всем ключевым метрикам качества.
Наиболее эффективной моделью в рамках проведённого эксперимента оказался Gradient Boosting, показавший лучшие значения ROC-AUC и PR-AUC, что свидетельствует о наилучшей способности модели различать классы и корректно ранжировать объекты. Но также важно отметить, что с каждой новой моделью (последовательность как в сетке) качество растёт.

```
Ссылка на лучший запуск 
http://158.160.2.37:5000/#/experiments/15/runs/bcbcc5a8281848fbb0b3bb3a56eb0396
```


### Эксперимент 3
Как и выяснилось в прошлых экспериментах лог. регрессия даёт результаты сильно ниже, чем более сложные модели. Поэтому я хочу попробовать применить разные методы оптимизации и посмотреть, как это повлияет на результат. 


**Гипотеза:** разные алгоритмы оптимизации по-разному находят минимум функции потерь, поэтому выбор solver влияет на качество модели.

**Исследуемый параметр:** solver в logreg

**Сетка изменения параметра:** lbfgs -> liblinear -> saga -> sag

#### Анализ результатов
Результаты эксперимента показывают, что выбор алгоритма оптимизации оказывает минимальное влияние на итоговое качество модели.
Все рассмотренные методы оптимизации демонстрируют практически одинаковые значения ключевых метрик:
Accuracy остаётся стабильной (около 0.751–0.752);
ROC-AUC изменяется незначительно и находится около 0.749;
PR-AUC также практически не изменяется;
значения precision, recall и F1-score отличаются лишь незначительно.
Небольшие различия наблюдаются в балансе precision и recall, однако они не приводят к существенному изменению качества модели в целом.
Это объясняется тем, что все рассмотренные solver’ы оптимизируют одну и ту же функцию потерь и сходятся к близким значениям параметров модели. Следовательно, при фиксированных данных и гиперпараметрах итоговое качество определяется скорее ограничениями самой модели (линейной зависимостью), чем методом оптимизации. 

### Эксперимент 4
В этом эксперименте я хочу попробовать улучшить самую хорошую по результатам модель.
Самый лучший результат был получен при таких параметрах 

model_params
{'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 3}
train_size: full
model_type: gradient_boosting
features: [race,sex,native.country,hours.per.week,occupation,marital.status,education]

**Гипотеза:** использование полного набора фичей даст моделе больше данных для обучения, а значит качество вырастит.

**Исследуемый параметр:** features

**Сетка изменения параметра:** в рамках данного эксперимента будет сравниваться результат gradient boosting из 2 экперимента без полного набора фичей и использование полного набора фичей при прочих равных.

#### Анализ результатов
Гипотеза подтвердилась, использование полного набора фичей даёт сильный прирост к качеству модели. Из всех экспериментов, этот показал лучший результат.

```
Ссылка на лучший запуск 
http://158.160.2.37:5000/#/experiments/15/runs/8abc06d74a3d4439ad611378c04c7990
```