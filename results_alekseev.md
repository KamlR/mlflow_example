### Вступление
В рамках анализа работы другого обучающегося, я взяла работу студента Алексеева. http://158.160.2.37:5000/#/experiments/5?searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D

В рамках данного пункта мне было интересно рассмотреть эксперименты, которые отличаются по разрезам от моих.
Ниже опишу три эксперимента, которые были проведенны студентом Алексеевым.

### Эксперимент 1
Он состоял из 3 запусков
```
Первый Feat_All_XGBoost_FullData
http://158.160.2.37:5000/#/experiments/5/runs/1ae8980e443145829acbfa15562a8c4e
```

```
Второй Feat_CoreDemographics_XGBoost_FullData
http://158.160.2.37:5000/#/experiments/5/runs/df03a48cbda4448a985fd586be4259cb
```

```
Третий Feat_NumericOnly_XGBoost_FullData
http://158.160.2.37:5000/#/experiments/5/runs/29926b13ec2e4391b59a02c66cd292fe
```

### Суть эксперимента
- Фиксируем обучающая выборку (12000)
- Берём модель XGBoost c max_depth = 4, random_state = 42, n_estimators = 100, learning_rate = 0.1
И далее начинаем смотреть, как обучится модель при выборе разных признаков.

### Гипотеза 
Следующие наборы фичей влияют на качество модели:
```
['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']


['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week', 'education', 'marital.status', 'sex', 'race']

['age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'capital.gain', 'capital.loss', 'hours.per.week', 'native.country']
```

### Анализ результатов
Гипотеза подтвердилась, использование вышеуказанных наборов фичей влияет на качество модели.
Причём использование только numeric features даёт самые низкие показатели, оно и логично, так как мы теряем достаточно большое кол-во информации, и модель хуже улавливает зависимости.

При этом самое хорошее качество по всем метрикам мы получаем в случае использования всего набора фичей.

Ссылка на лучший результат 
```
Третий Feat_NumericOnly_XGBoost_FullData
http://158.160.2.37:5000/#/experiments/5/runs/29926b13ec2e4391b59a02c66cd292fe
```


### Эксперимент 2

Он состоял из 2 запусков
```
Model_LogReg_C=0.1_AllFeat_FullData
http://158.160.2.37:5000/#/experiments/5/runs/1d62d99592944de1a463d442e6a49396
```

```
Второй Model_LogReg_C=10.0_AllFeat_FullData
http://158.160.2.37:5000/#/experiments/5/runs/42333b70a65c4796a3e048880d26b60f
```

### Суть эксперимента
Берём модель лог. регрессии с random_state=42, solver = lbfgs, max_iter = 1000.
train_size = 12000 и все признаки.

### Гипотеза 
Маленькое значение C (сильная регуляризация) и большое значение C (слабая регуляризация) влияют на качество модели.

### Сетка изменений
Для C было проведено два эксперимента, а именно C = 0.1 и C = 10.0

### Анализ результатов
Гипотеза не подтвердилась, метрики качества в обоих случаях +- одинаковые, сильных расхождений не наблюдается.

### Эксперимент 3
Он состоял из 3 запусков
```
Первый Size_5000_XGBoost_Log_Dataset
http://158.160.2.37:5000/#/experiments/5/runs/ff9547da3a764149bf9b669d15b180f8
```

```
Второй Size_12000_XGBoost_Log_Dataset
http://158.160.2.37:5000/#/experiments/5/runs/6aaccf44cc4441e08256448b34070719
```

```
Третий Size_Full_XGBoost_Log_Dataset
http://158.160.2.37:5000/#/experiments/5/runs/f8590223a90440769a0f9fe8578a6048
```
### Суть эксперимента
Берём модель XGBoost с max_depth = 5, random_state = 42, n_estimators = 150, learning_rate = 0.1
Используем все фичи

### Гипотеза 
Размер обучающей выборки (train_size) влияет на качество модели.

### Сетка изменений
5000 -> 12000 -> full_data

### Анализ результатов
Гипотеза подтвердилась. Размер обучающей выборки и правда влияет на качество обучения выбранной модели. Но важно отметить, что заметная разница наблюдается между 5000 и full_data, в то время как 12000 уже не сильно отличается от full train_size. Вероятно, это связано с тем, что при 12000 модель уже имеет достаточное кол-во информации для корректных предсказаний, и дальнейшее увеличении обучающей выборки не даёт сильного вклада.

```
Ссылка на лучший результат: 
http://158.160.2.37:5000/#/experiments/5/runs/f8590223a90440769a0f9fe8578a6048
```